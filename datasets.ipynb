{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T12:12:16.141513Z",
     "start_time": "2025-10-29T12:12:11.155260Z"
    }
   },
   "source": "from datasets import *",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:14:10.410941Z",
     "start_time": "2025-10-29T12:13:44.889820Z"
    }
   },
   "cell_type": "code",
   "source": "datasets = load_dataset(\"madao33/new-title-chinese\")",
   "id": "e50538a926678ae9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/madao33/new-title-chinese/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1147)')))\"), '(Request ID: 9a70e190-1181-4e4e-b2d8-bc49f2d6e80c)')' thrown while requesting HEAD https://huggingface.co/datasets/madao33/new-title-chinese/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/madao33/new-title-chinese/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1147)')))\"), '(Request ID: 7cb6473e-38ed-490e-88d1-225f6d0aa6e6)')' thrown while requesting HEAD https://huggingface.co/datasets/madao33/new-title-chinese/resolve/main/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/madao33/new-title-chinese/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1147)')))\"), '(Request ID: ca0a2493-def4-4a6a-9b72-1dfebfa6ff33)')' thrown while requesting HEAD https://huggingface.co/datasets/madao33/new-title-chinese/resolve/main/README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/madao33/new-title-chinese/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1147)')))\"), '(Request ID: 749afd03-6487-42cc-800c-f36951567937)')' thrown while requesting HEAD https://huggingface.co/datasets/madao33/new-title-chinese/resolve/main/README.md\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/madao33/new-title-chinese/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1147)')))\"), '(Request ID: a30f7364-aef2-4002-885a-3b8856c8332f)')' thrown while requesting HEAD https://huggingface.co/datasets/madao33/new-title-chinese/resolve/main/README.md\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/madao33/new-title-chinese/resolve/main/README.md (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1147)')))\"), '(Request ID: 49f5419d-f033-427e-b311-aacbd774a3b9)')' thrown while requesting HEAD https://huggingface.co/datasets/madao33/new-title-chinese/resolve/main/README.md\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'madao33/new-title-chinese' on the Hub (SSLError)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m datasets \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmadao33/new-title-chinese\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\datasets\\load.py:1397\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001B[0m\n\u001B[0;32m   1392\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   1393\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   1394\u001B[0m )\n\u001B[0;32m   1396\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 1397\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   1398\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   1399\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   1400\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   1401\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   1402\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   1403\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   1404\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   1405\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   1406\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   1407\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   1408\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   1409\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   1410\u001B[0m )\n\u001B[0;32m   1412\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   1413\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\datasets\\load.py:1137\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001B[0m\n\u001B[0;32m   1135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1136\u001B[0m     features \u001B[38;5;241m=\u001B[39m _fix_for_backward_compatible_features(features)\n\u001B[1;32m-> 1137\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;66;03m# Get dataset builder class\u001B[39;00m\n\u001B[0;32m   1147\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\datasets\\load.py:1036\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001B[0m\n\u001B[0;32m   1031\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e1, \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m):\n\u001B[0;32m   1032\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[0;32m   1033\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find any data file at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1034\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m on the Hugging Face Hub either: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(e1)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1035\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1036\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e1 \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1038\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find any data file at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrelative_to_absolute_path(path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\datasets\\load.py:972\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001B[0m\n\u001B[0;32m    960\u001B[0m     commit_hash \u001B[38;5;241m=\u001B[39m api\u001B[38;5;241m.\u001B[39mdataset_info(\n\u001B[0;32m    961\u001B[0m         path,\n\u001B[0;32m    962\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m    963\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100.0\u001B[39m,\n\u001B[0;32m    964\u001B[0m     )\u001B[38;5;241m.\u001B[39msha\n\u001B[0;32m    965\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[0;32m    966\u001B[0m     OfflineModeIsEnabled,\n\u001B[0;32m    967\u001B[0m     requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mTimeout,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    970\u001B[0m     httpx\u001B[38;5;241m.\u001B[39mTimeoutException,\n\u001B[0;32m    971\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 972\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt reach \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m on the Hub (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    974\u001B[0m     message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is a gated dataset on the Hub.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mConnectionError\u001B[0m: Couldn't reach 'madao33/new-title-chinese' on the Hub (SSLError)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained()\n",
    "def preprocess_function(example):\n",
    "    model_input = tokenizer(example[\"title\"], truncation=True, max_length=512)\n",
    "    labels = tokenizer(example[\"title\"], truncation=True, max_length=512)\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input"
   ],
   "id": "fc17c2dcfb1c133f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#加载本地数据集\n",
    "datasets = load_dataset(\"类型\",data_files = \"path\",split = \"train\")\n",
    "#加载文件夹内全部文件作为数据集\n",
    "datasets = load_dataset(\"\",data_dir=\"path\",split = \"train\")\n",
    "\n",
    "#通过预先加载的其他格式转换加载数据集\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"path/train.csv\")\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n"
   ],
   "id": "db233a2b4e67f4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "dataset = load_dataset()\n",
    "dataset = dataset.filter(lambda x :x[\"\"] is not None)\n",
    "def process_function(example):\n",
    "    tokenizer_example = tokenizer(example[\"review\"], truncation=True, max_length=512)\n",
    "    tokenizer_example[\"label\"] = example[\"labels\"]\n",
    "    return tokenizer_example\n",
    "tokenizer_dataset = datasets.map(process_function, batched=True,max_length=128,truncation=True)\n",
    "collater = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(tokenizer_dataset,collate_fn=collater,batch_size=32,shuffle=True)\n"
   ],
   "id": "4bd5944872564d80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
