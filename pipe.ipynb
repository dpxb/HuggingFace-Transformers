{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T02:16:59.425053Z",
     "start_time": "2025-10-29T02:16:59.411672Z"
    }
   },
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:12:09.525463Z",
     "start_time": "2025-10-29T02:12:09.516580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for k,v in SUPPORTED_TASKS.items():\n",
    "    print(k,v[\"type\"])"
   ],
   "id": "caa8d199121872f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-classification audio\n",
      "automatic-speech-recognition multimodal\n",
      "text-to-audio text\n",
      "feature-extraction multimodal\n",
      "text-classification text\n",
      "token-classification text\n",
      "question-answering text\n",
      "table-question-answering text\n",
      "visual-question-answering multimodal\n",
      "document-question-answering multimodal\n",
      "fill-mask text\n",
      "summarization text\n",
      "translation text\n",
      "text2text-generation text\n",
      "text-generation text\n",
      "zero-shot-classification text\n",
      "zero-shot-image-classification multimodal\n",
      "zero-shot-audio-classification multimodal\n",
      "image-classification image\n",
      "image-feature-extraction image\n",
      "image-segmentation multimodal\n",
      "image-to-text multimodal\n",
      "image-text-to-text multimodal\n",
      "object-detection multimodal\n",
      "zero-shot-object-detection multimodal\n",
      "depth-estimation image\n",
      "video-classification video\n",
      "mask-generation multimodal\n",
      "image-to-image image\n",
      "keypoint-matching image\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:12:09.885086Z",
     "start_time": "2025-10-29T02:12:09.871703Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import *",
   "id": "e0255c2d3831aa5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:12:10.978323Z",
     "start_time": "2025-10-29T02:12:10.253606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"D:\\Hugging Face Hub\",  # 替换为你的本地文件夹路径\n",
    "    tokenizer=\"D:\\Hugging Face Hub\"  # 分词器与模型同路径\n",
    ")"
   ],
   "id": "2b1e9695f2b4cb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file D:\\Hugging Face Hub\\pytorch_model.bin\n",
      "Since the `dtype` attribute can't be found in model's config object, will use dtype={dtype} as derived from model's weights\n",
      "Instantiating BertForSequenceClassification model under default dtype torch.float32.\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:12:11.041518Z",
     "start_time": "2025-10-29T02:12:10.982872Z"
    }
   },
   "cell_type": "code",
   "source": "pipe(\"This restaurant has amazing food and friendly staff!\")",
   "id": "e0a50cf4a334540a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive (stars 4 and 5)', 'score': 0.8685020208358765}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:17:30.716427Z",
     "start_time": "2025-10-29T02:17:30.571744Z"
    }
   },
   "cell_type": "code",
   "source": "pipe(\"这家餐厅菜太咸了，不会来了\")",
   "id": "e0a9a3bb4da515c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative (stars 1, 2 and 3)', 'score': 0.990542471408844}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:21:06.490498Z",
     "start_time": "2025-10-29T02:21:06.479503Z"
    }
   },
   "cell_type": "code",
   "source": "pipe.model.device",
   "id": "4ab64e8d74792c83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:25:15.373642Z",
     "start_time": "2025-10-29T02:25:14.642080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa_pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"D:\\Hugging Face Hub\",  # 替换为你的本地文件夹路径\n",
    "    tokenizer=\"D:\\Hugging Face Hub\"  # 分词器与模型同路径\n",
    ")"
   ],
   "id": "c081c6a0c45aa25d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file D:\\Hugging Face Hub\\pytorch_model.bin\n",
      "Since the `dtype` attribute can't be found in model's config object, will use dtype={dtype} as derived from model's weights\n",
      "Instantiating BertForQuestionAnswering model under default dtype torch.float32.\n",
      "Some weights of the model checkpoint at D:\\Hugging Face Hub were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at D:\\Hugging Face Hub and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"auto\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"auto\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:25:42.649951Z",
     "start_time": "2025-10-29T02:25:42.644623Z"
    }
   },
   "cell_type": "code",
   "source": "qa_pipe",
   "id": "698cc84f6bd480df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x1d11b03aa60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:32:03.737977Z",
     "start_time": "2025-10-29T02:32:03.699070Z"
    }
   },
   "cell_type": "code",
   "source": "qa_pipe(question=\"中国的首都是哪里?\",context=\"中国的首都是北京\")",
   "id": "77c098c6cad9fd75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.015118112787604332, 'start': 7, 'end': 8, 'answer': '京'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T02:37:20.468304Z",
     "start_time": "2025-10-29T02:37:19.921274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detector = pipeline(\n",
    "    \"zero-shot-object-detection\",\n",
    "    model=\"D:\\Hugging Face Hub\",  # 替换为你的本地文件夹路径\n",
    "    tokenizer=\"D:\\Hugging Face Hub\"  # 分词器与模型同路径\n",
    ")"
   ],
   "id": "ca45434ceeb370ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file D:\\Hugging Face Hub\\pytorch_model.bin\n",
      "Since the `dtype` attribute can't be found in model's config object, will use dtype={dtype} as derived from model's weights\n",
      "Instantiating BertForSequenceClassification model under default dtype torch.float32.\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
      "    \"1\": \"positive (stars 4 and 5)\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative (stars 1, 2 and 3)\": 0,\n",
      "    \"positive (stars 4 and 5)\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file D:\\Hugging Face Hub\\config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for 'D:\\Hugging Face Hub'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'D:\\Hugging Face Hub' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\transformers\\image_processing_base.py:354\u001B[0m, in \u001B[0;36mImageProcessingMixin.get_image_processor_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    333\u001B[0m     resolved_image_processor_files \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    334\u001B[0m         resolved_file\n\u001B[0;32m    335\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m [image_processor_file, PROCESSOR_NAME]\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    352\u001B[0m         \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    353\u001B[0m     ]\n\u001B[1;32m--> 354\u001B[0m     resolved_image_processor_file \u001B[38;5;241m=\u001B[39m \u001B[43mresolved_image_processor_files\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m    356\u001B[0m     \u001B[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001B[39;00m\n\u001B[0;32m    357\u001B[0m     \u001B[38;5;66;03m# the original exception.\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m detector \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mzero-shot-object-detection\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mD:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mHugging Face Hub\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 替换为你的本地文件夹路径\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mD:\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mHugging Face Hub\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 分词器与模型同路径\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1109\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1108\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m load_image_processor:\n\u001B[1;32m-> 1109\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m   1110\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1111\u001B[0m         image_processor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1104\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m   1102\u001B[0m     \u001B[38;5;66;03m# Instantiate image_processor if needed\u001B[39;00m\n\u001B[0;32m   1103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(image_processor, (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m-> 1104\u001B[0m         image_processor \u001B[38;5;241m=\u001B[39m AutoImageProcessor\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m   1105\u001B[0m             image_processor, _from_pipeline\u001B[38;5;241m=\u001B[39mtask, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs\n\u001B[0;32m   1106\u001B[0m         )\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1108\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m load_image_processor:\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:494\u001B[0m, in \u001B[0;36mAutoImageProcessor.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    491\u001B[0m     \u001B[38;5;66;03m# In case we have a config_dict, but it's not a timm config dict, we raise the initial exception,\u001B[39;00m\n\u001B[0;32m    492\u001B[0m     \u001B[38;5;66;03m# because only timm models have image processing in `config.json`.\u001B[39;00m\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_timm_config_dict(config_dict):\n\u001B[1;32m--> 494\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m initial_exception\n\u001B[0;32m    496\u001B[0m image_processor_type \u001B[38;5;241m=\u001B[39m config_dict\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_processor_type\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    497\u001B[0m image_processor_auto_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:476\u001B[0m, in \u001B[0;36mAutoImageProcessor.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;66;03m# Load the image processor config\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;66;03m# Main path for all transformers models and local TimmWrapper checkpoints\u001B[39;00m\n\u001B[1;32m--> 476\u001B[0m     config_dict, _ \u001B[38;5;241m=\u001B[39m ImageProcessingMixin\u001B[38;5;241m.\u001B[39mget_image_processor_dict(\n\u001B[0;32m    477\u001B[0m         pretrained_model_name_or_path, image_processor_filename\u001B[38;5;241m=\u001B[39mimage_processor_filename, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    478\u001B[0m     )\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m initial_exception:\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;66;03m# Fallback path for Hub TimmWrapper checkpoints. Timm models' image processing is saved in `config.json`\u001B[39;00m\n\u001B[0;32m    481\u001B[0m     \u001B[38;5;66;03m# instead of `preprocessor_config.json`. Because this is an Auto class and we don't have any information\u001B[39;00m\n\u001B[0;32m    482\u001B[0m     \u001B[38;5;66;03m# except the model name, the only way to check if a remote checkpoint is a timm model is to try to\u001B[39;00m\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;66;03m# load `config.json` and if it fails with some error, we raise the initial exception.\u001B[39;00m\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda\\ana\\envs\\PyTorch\\lib\\site-packages\\transformers\\image_processing_base.py:361\u001B[0m, in \u001B[0;36mImageProcessingMixin.get_image_processor_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    358\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    360\u001B[0m         \u001B[38;5;66;03m# For any other exception, we throw a generic error.\u001B[39;00m\n\u001B[1;32m--> 361\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    362\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load image processor for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. If you were trying to load\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    363\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m it from \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, make sure you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have a local directory with the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    364\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m same name. Otherwise, make sure \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is the correct path to a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    365\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m directory containing a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage_processor_filename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m file\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    366\u001B[0m         )\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m     \u001B[38;5;66;03m# Load image_processor dict\u001B[39;00m\n\u001B[0;32m    370\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(resolved_image_processor_file, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m reader:\n",
      "\u001B[1;31mOSError\u001B[0m: Can't load image processor for 'D:\\Hugging Face Hub'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'D:\\Hugging Face Hub' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd688af07f3f434a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
