{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-05T03:04:47.562257Z",
     "start_time": "2025-11-05T03:04:47.551601Z"
    }
   },
   "source": [
    "#导入相关的包\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T03:04:51.179864Z",
     "start_time": "2025-11-05T03:04:50.006301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#加载数据集\n",
    "dataset = load_dataset(\"json\",data_files=\"./train_pair_1w.json\",split=\"train\")\n",
    "dataset"
   ],
   "id": "944029fae2301ab7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T03:04:52.445627Z",
     "start_time": "2025-11-05T03:04:52.421681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#划分数据集\n",
    "datasets = dataset.train_test_split(test_size=0.2)\n"
   ],
   "id": "7428206bc55ce4cc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T08:39:33.924136Z",
     "start_time": "2025-11-04T08:39:31.851648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#数据集预处理\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:\\Hugging Face Hub\\chinese-macbert-base\")\n",
    "def process_function(examples):\n",
    "    sentence = []\n",
    "    labels = []\n",
    "    for sen1,sen2,label in zip(examples[\"sentence1\"],examples[\"sentence2\"],examples[\"label\"]):\n",
    "        sentence.append(sen1)\n",
    "        sentence.append(sen2)\n",
    "        labels.append( 1 if int(label)==1 else -1)\n",
    "    tokenizer_examples = tokenizer(sentence, truncation=True, max_length=128,padding =\"max_length\")\n",
    "    tokenizer_examples = {k:[v[i:i+2] for i in range(0,len(v),2)] for k,v in tokenizer_examples.items()}\n",
    "    tokenizer_examples[\"label\"] = labels\n",
    "    return tokenizer_examples\n",
    "tokenizer_datasets = datasets.map(process_function, batched=True,remove_columns=datasets[\"train\"].column_names)"
   ],
   "id": "a406433496ae92b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd4e458536564394bbc12ae0ebedc113"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39e7aeef07bb47378e3d6cf69d6dc7c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T08:39:34.603701Z",
     "start_time": "2025-11-04T08:39:33.928728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#创建模型\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"D:\\Hugging Face Hub\\chinese-macbert-base\",num_labels=1)\n",
    "from transformers import BertForSequenceClassification,BertPreTrainedModel,BertModel\n",
    "from typing import Optional\n",
    "from torch.nn import CosineSimilarity,CosineEmbeddingLoss\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "class Dualmodel(BertPreTrainedModel):\n",
    "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.post_init()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        #获得句子A和句子B的输入\n",
    "        senA_input_ids,senB_input_ids = input_ids[:,0],input_ids[:,1]\n",
    "        senA_attention_mask,senB_attention_mask = attention_mask[:,0],attention_mask[:,1]\n",
    "        senA_token_type_ids,senB_token_type_ids = token_type_ids[:,0],token_type_ids[:,1]\n",
    "\n",
    "        #获取两个句子的向量表示\n",
    "        senA_outputs = self.bert(\n",
    "            senA_input_ids,\n",
    "            attention_mask=senA_attention_mask,\n",
    "            token_type_ids=senA_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        pooledA_output = senA_outputs[1]\n",
    "        senB_outputs = self.bert(\n",
    "                        senB_input_ids,\n",
    "                        attention_mask=senB_attention_mask,\n",
    "                        token_type_ids=senB_token_type_ids,\n",
    "                        position_ids=position_ids,\n",
    "                        head_mask=head_mask,\n",
    "                        inputs_embeds=inputs_embeds,\n",
    "                        output_attentions=output_attentions,\n",
    "                        output_hidden_states=output_hidden_states,\n",
    "                        return_dict=return_dict,\n",
    "        )\n",
    "        pooledB_output = senB_outputs[1]\n",
    "        #计算相似度\n",
    "        cos = CosineSimilarity()(pooledA_output,pooledB_output)\n",
    "        #计算loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CosineEmbeddingLoss(0.3)\n",
    "            loss = loss_fct(pooledA_output,pooledB_output,labels)\n",
    "        output = (cos,)\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "model = Dualmodel.from_pretrained(\"D:\\Hugging Face Hub\\chinese-macbert-base\")"
   ],
   "id": "18647bb195df4507",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T08:39:34.789649Z",
     "start_time": "2025-11-04T08:39:34.620398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#创建评估函数\n",
    "import evaluate\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def eval_metric(eval_predict):\n",
    "    predictions,labels = eval_predict\n",
    "    predictions =[int(i>0.7) for i in predictions]\n",
    "    labels =[int(i>0) for i in labels]\n",
    "    #predictions = predictions.argmax(dim=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, labels=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, labels=labels)\n",
    "    acc.update(f1)\n",
    "    return acc\n"
   ],
   "id": "6dfb781ebbaed305",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T08:39:34.930208Z",
     "start_time": "2025-11-04T08:39:34.806838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#创建TrainingArguments\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./dual_model\",\n",
    "    per_gpu_eval_batch_size=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    ")"
   ],
   "id": "ab667379acf5d7da",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T08:39:35.347160Z",
     "start_time": "2025-11-04T08:39:34.947134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#创建Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenizer_datasets[\"train\"],\n",
    "    eval_dataset=tokenizer_datasets[\"test\"],\n",
    "    #data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=eval_metric,\n",
    ")\n"
   ],
   "id": "6b11f30b0e7804dd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T09:00:00.834995Z",
     "start_time": "2025-11-04T08:39:35.364480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#模型的训练\n",
    "trainer.train()"
   ],
   "id": "aaa595c26eff24a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 20:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.16036114438374838, metrics={'train_runtime': 1225.3264, 'train_samples_per_second': 19.587, 'train_steps_per_second': 4.897, 'total_flos': 3157275967488000.0, 'train_loss': 0.16036114438374838, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T09:00:00.897692Z",
     "start_time": "2025-11-04T09:00:00.878942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#模型预测\n",
    "class SentenceSimilarityPipeline:\n",
    "\n",
    "    def __init__(self, model, tokenizer) -> None:\n",
    "        self.model = model.bert\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    def preprocess(self, senA, senB):\n",
    "        return self.tokenizer([senA, senB], max_length=128, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        return self.model(**inputs)[1]  # [2, 768]\n",
    "\n",
    "    def postprocess(self, logits):\n",
    "        cos = CosineSimilarity()(logits[None, 0, :], logits[None,1, :]).squeeze().cpu().item()\n",
    "        return cos\n",
    "\n",
    "    def __call__(self, senA, senB, return_vector=False):\n",
    "        inputs = self.preprocess(senA, senB)\n",
    "        logits = self.predict(inputs)\n",
    "        result = self.postprocess(logits)\n",
    "        if return_vector:\n",
    "            return result, logits\n",
    "        else:\n",
    "            return result"
   ],
   "id": "d981e99b570a93a9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T09:00:00.913350Z",
     "start_time": "2025-11-04T09:00:00.903191Z"
    }
   },
   "cell_type": "code",
   "source": "pipe = SentenceSimilarityPipeline(model,tokenizer)\n",
   "id": "b2473dc247ce10e3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T09:00:00.959397Z",
     "start_time": "2025-11-04T09:00:00.929754Z"
    }
   },
   "cell_type": "code",
   "source": "pipe(\"我喜欢北京\",\"北京是个好地方，我想下次再来\")",
   "id": "807eb982a9b27814",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6053197383880615"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T09:00:01.036304Z",
     "start_time": "2025-11-04T09:00:01.032137Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f3ee3ba4819c3ab7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "162a6d3e12f59c59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
